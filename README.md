# ‚ôüÔ∏è Hive AlphaZero Agent  
*A deep reinforcement learning framework for the board game Hive, inspired by AlphaZero.*

---

## üéØ Overview  
This project was developed within the **Scuola Ortogonale Program** by the **Elicsir Foundation**, an initiative supporting selected students from Italian universities in their academic journey.  

The goal was to design an agent capable of playing **Hive** competitively ‚Äî first outperforming a random player, then reaching human-level play ‚Äî by combining **Monte Carlo Tree Search (MCTS)** with **deep reinforcement learning**, following a simplified **AlphaZero-style** approach.

The system re-implements the *AlphaZero* pipeline (*self-play ‚Üí neural network training ‚Üí arena evaluation*) adapted to Hive‚Äôs unique rules and **hexagonal topology**, using the **Mzinga** engine as the backend for rule enforcement and move validation.

---

## ‚öôÔ∏è Architecture  
- **Search:** Monte Carlo Tree Search with upper confidence bound (PUCT) exploration.  
- **Policy & Value Network:** 8-layer CNN operating on a 4-plane binary board encoding (queen/others √ó white/black), with tile-relative move encoding.  
- **Training Loop:** Alternates between *self-play* and *supervised updates* of the network using cross-entropy (policy) and MSE (value) losses.  
- **Self-Play Parallelization:** Multi-process CPU pool generating episodes concurrently, with **GPU inference servers** for batched network evaluations.  
- **Distributed Execution:** Designed to run efficiently on multi-GPU clusters (DISI HPC), with synchronized queues and fault-tolerant inference.

---

## üß† Learning Pipeline  

### 1Ô∏è‚É£ Pre-Training  
The network is first trained on a corpus of **5k human games** (2020‚Äì2025) using supervised imitation learning.  
- Parallel parsing and sharded datasets.  
- Data augmentation via board symmetries (rotations, flips).  
- Efficient loaders with `uint8` / `float16` compression for large-scale training.

### 2Ô∏è‚É£ Self-Play Reinforcement Learning  
- Multiple CPU workers simulate matches using the current policy/value network.  
- GPU inference servers compute batched predictions.  
- Training data generated in continuous cycles.  
- Arena evaluation retains the best-performing model.

### 3Ô∏è‚É£ Containerized Deployment  
The final trained agent (`best.pth.tar`) is packaged in a **Docker container** with the Mzinga engine for reproducible evaluation.

---

## üìà Results  
- Stable end-to-end training pipeline (pre-training ‚Üí self-play ‚Üí evaluation).  
- Generated hundreds of thousands of self-play examples within 24 hours on the cluster.  
- The network learns non-trivial positional heuristics and achieves a **~60% win rate** vs. random agent.  
- Awarded the **‚ÄúLee Sedol Prize‚Äù** by the *Elicsir Foundation* for innovative use of reinforcement learning in Hive.

---

## üß© Implementation Highlights  

| Component | Description |
|------------|-------------|
| **Board Encoding** | 4-plane binary representation (queen/others √ó color). |
| **Move Encoding** | Tile-relative indexing (28 √ó 14 √ó 7 + 1 = 2745 possible moves). |
| **Network** | 8-layer CNN predicting policy and value jointly. |
| **Optimization** | Adam optimizer, learning rate scheduling, early stopping on arena results. |
| **Parallelism** | Multiprocessing + GPU inference queues (NCCL/Gloo backend). |
| **Frameworks** | PyTorch ‚Ä¢ AlphaZero General ‚Ä¢ Mzinga ‚Ä¢ Docker ‚Ä¢ Python 3.10 |

---

## üöÄ Running the Project  

```bash
# Create environment
conda create -n hive_rl python=3.10
conda activate hive_rl
pip install -r requirements.txt

# Run self-play and training loop
python main.py

```

---

## üìö References  
- Goede et al., *The Cost of Reinforcement Learning for Game Engines: The AZ-Hive Case Study (2022).*  
- Surag Nair, *AlphaZero General Framework.*  
- Jon Thysell, *Mzinga: Open-Source Hive Engine.*

---

## üë• Authors  
**Nancy Kalaj** & **Ludovico Cappellato**  
Developed for the *Scuola Ortogonale 2024/25* by the *Elicsir Foundation*.  
Awarded the **‚ÄúLee Sedol Prize‚Äù** for innovative use of deep reinforcement learning.
